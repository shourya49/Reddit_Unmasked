{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29748730-e0a9-4c67-b427-e9fe5b5200fb",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6cb6f-97df-40d1-b768-4074ebdd299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install httpx loguru parsel pandas requests beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53001367-14b5-4447-a2c6-a98792e92391",
   "metadata": {},
   "source": [
    "# Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e35f94b1-a1c3-4baa-9aa5-7db7591d4b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "from typing import List, Dict, Union\n",
    "from datetime import datetime, timezone\n",
    "from httpx import AsyncClient, Response\n",
    "from loguru import logger as log\n",
    "from parsel import Selector\n",
    "import logging\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from httpx import AsyncClient, Response\n",
    "from parsel import Selector\n",
    "from typing import List, Dict, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b0284-ff89-4a2f-99cb-f65f4b1bc2ce",
   "metadata": {},
   "source": [
    "# Fetch the post of the User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9397b94-13bd-452a-8123-e904612fe4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Minimal logger\n",
    "class log:\n",
    "    @staticmethod\n",
    "    def success(msg): print(\"[SUCCESS]\", msg)\n",
    "    @staticmethod\n",
    "    def error(msg): print(\"[ERROR]\", msg)\n",
    "\n",
    "client = AsyncClient(\n",
    "    headers={\"User-Agent\": \"Mozilla/5.0\"},\n",
    "    timeout=10.0,\n",
    "    follow_redirects=True\n",
    ")\n",
    "from typing import List, Dict, Union\n",
    "from httpx import Response\n",
    "\n",
    "def parse_user_posts(response: Response) -> Dict:\n",
    "    selector = Selector(response.text)\n",
    "    data = []\n",
    "    for box in selector.xpath(\"//div[@id='siteTable']/div[contains(@class, 'thing')]\"):\n",
    "        author = box.xpath(\"./@data-author\").get()\n",
    "        link = box.xpath(\"./@data-permalink\").get()\n",
    "        publishing_date = box.xpath(\"./@data-timestamp\").get()\n",
    "        publishing_date = (\n",
    "            datetime.fromtimestamp(int(publishing_date) / 1000.0, tz=timezone.utc).isoformat()\n",
    "            if publishing_date else None\n",
    "        )\n",
    "        comment_count = box.xpath(\"./@data-comments-count\").get()\n",
    "        post_score = box.xpath(\"./@data-score\").get()\n",
    "        data.append({\n",
    "            \"authorId\": box.xpath(\"./@data-author-fullname\").get(),\n",
    "            \"author\": author,\n",
    "            \"authorProfile\": f\"https://www.reddit.com/user/{author}\" if author else None,\n",
    "            \"postId\": box.xpath(\"./@data-fullname\").get(),\n",
    "            \"postLink\": f\"https://www.reddit.com{link}\" if link else None,\n",
    "            \"postTitle\": box.xpath(\".//p[@class='title']/a/text()\").get(),\n",
    "            \"postSubreddit\": box.xpath(\"./@data-subreddit-prefixed\").get(),\n",
    "            \"publishingDate\": publishing_date,\n",
    "            \"commentCount\": int(comment_count) if comment_count else None,\n",
    "            \"postScore\": int(post_score) if post_score else None,\n",
    "            \"attachmentType\": box.xpath(\"./@data-type\").get(),\n",
    "            \"attachmentLink\": box.xpath(\"./@data-url\").get(),\n",
    "        })\n",
    "    next_page_url = selector.xpath(\"//span[@class='next-button']/a/@href\").get()\n",
    "    return {\"data\": data, \"url\": next_page_url}\n",
    "\n",
    "async def scrape_user_posts(username: str, sort: Union[\"new\", \"top\", \"controversial\"], max_pages: int = None) -> List[Dict]:\n",
    "    url = f\"https://old.reddit.com/user/{username}/submitted/?sort={sort}\"\n",
    "    response = await client.get(url)\n",
    "    data = parse_user_posts(response)\n",
    "    post_data, next_page_url = data[\"data\"], data[\"url\"]\n",
    "\n",
    "    visited_urls = set()\n",
    "    while next_page_url and (max_pages is None or max_pages > 0):\n",
    "        if next_page_url in visited_urls:\n",
    "            break\n",
    "        visited_urls.add(next_page_url)\n",
    "\n",
    "        response = await client.get(next_page_url)\n",
    "        data = parse_user_posts(response)\n",
    "        next_page_url = data[\"url\"]\n",
    "        post_data.extend(data[\"data\"])\n",
    "        if max_pages is not None:\n",
    "            max_pages -= 1\n",
    "\n",
    "    log.success(f\"Scraped {len(post_data)} posts from {username}'s Reddit profile\")\n",
    "    return post_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7c7024-377d-40e1-bd9c-77891d718f17",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2df60-2790-416d-8597-c6d09b8c67bb",
   "metadata": {},
   "source": [
    "## Provide username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dd28254-14a4-498b-885f-d06a1f6e9f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Scraped 2 posts from Hungry-Move-6603's Reddit profile\n"
     ]
    }
   ],
   "source": [
    "reddit_username = \n",
    "posts = await scrape_user_posts(reddit_username, \"new\", max_pages=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6f1419d-8af0-4c6b-a3dc-73c0d237f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert it into a data frame\n",
    "df = pd.DataFrame(posts)\n",
    "df.head(5)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def fetch_reddit_post_text(post_url: str):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(post_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Generalized match: find div where ID matches \"t3_<something>-post-rtjson-content\"\n",
    "    post_div = soup.find(\"div\", id=re.compile(r\"t3_[\\w]+-post-rtjson-content\"))\n",
    "\n",
    "    if post_div:\n",
    "        post_paragraphs = post_div.find_all(\"p\")\n",
    "        post_text = \"\\n\\n\".join(p.get_text(strip=True) for p in post_paragraphs)\n",
    "        return post_text\n",
    "    else:\n",
    "        return \"❌ Post content not found.\"\n",
    "\n",
    "\n",
    "df['postContent'] = df['postLink'].apply(fetch_reddit_post_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77ec4269-04be-4797-b160-76e6806a2c48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authorId</th>\n",
       "      <th>author</th>\n",
       "      <th>authorProfile</th>\n",
       "      <th>postId</th>\n",
       "      <th>postLink</th>\n",
       "      <th>postTitle</th>\n",
       "      <th>postSubreddit</th>\n",
       "      <th>publishingDate</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>postScore</th>\n",
       "      <th>attachmentType</th>\n",
       "      <th>attachmentLink</th>\n",
       "      <th>postContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_bcxve1ah</td>\n",
       "      <td>Hungry-Move-6603</td>\n",
       "      <td>https://www.reddit.com/user/Hungry-Move-6603</td>\n",
       "      <td>t3_1lx50qm</td>\n",
       "      <td>https://www.reddit.com/r/lucknow/comments/1lx5...</td>\n",
       "      <td>Productive weekend activities in LKO?</td>\n",
       "      <td>r/lucknow</td>\n",
       "      <td>2025-07-11T12:00:04+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>link</td>\n",
       "      <td>/r/lucknow/comments/1lx50qm/productive_weekend...</td>\n",
       "      <td>❌ Post content not found.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_bcxve1ah</td>\n",
       "      <td>Hungry-Move-6603</td>\n",
       "      <td>https://www.reddit.com/user/Hungry-Move-6603</td>\n",
       "      <td>t3_1lwyhny</td>\n",
       "      <td>https://www.reddit.com/r/lucknow/comments/1lwy...</td>\n",
       "      <td>Everyone is something in LKO</td>\n",
       "      <td>r/lucknow</td>\n",
       "      <td>2025-07-11T05:12:13+00:00</td>\n",
       "      <td>94</td>\n",
       "      <td>222</td>\n",
       "      <td>link</td>\n",
       "      <td>/r/lucknow/comments/1lwyhny/everyone_is_someth...</td>\n",
       "      <td>Born and raised in Delhi - I shifted to LKO in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      authorId            author  \\\n",
       "0  t2_bcxve1ah  Hungry-Move-6603   \n",
       "1  t2_bcxve1ah  Hungry-Move-6603   \n",
       "\n",
       "                                  authorProfile      postId  \\\n",
       "0  https://www.reddit.com/user/Hungry-Move-6603  t3_1lx50qm   \n",
       "1  https://www.reddit.com/user/Hungry-Move-6603  t3_1lwyhny   \n",
       "\n",
       "                                            postLink  \\\n",
       "0  https://www.reddit.com/r/lucknow/comments/1lx5...   \n",
       "1  https://www.reddit.com/r/lucknow/comments/1lwy...   \n",
       "\n",
       "                               postTitle postSubreddit  \\\n",
       "0  Productive weekend activities in LKO?     r/lucknow   \n",
       "1           Everyone is something in LKO     r/lucknow   \n",
       "\n",
       "              publishingDate  commentCount  postScore attachmentType  \\\n",
       "0  2025-07-11T12:00:04+00:00             0          1           link   \n",
       "1  2025-07-11T05:12:13+00:00            94        222           link   \n",
       "\n",
       "                                      attachmentLink  \\\n",
       "0  /r/lucknow/comments/1lx50qm/productive_weekend...   \n",
       "1  /r/lucknow/comments/1lwyhny/everyone_is_someth...   \n",
       "\n",
       "                                         postContent  \n",
       "0                          ❌ Post content not found.  \n",
       "1  Born and raised in Delhi - I shifted to LKO in...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the data frame\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b83c9-1471-4ec2-aed8-7ffbe532d0af",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a0f3a4-0a28-4ecd-9b9b-5fff6120bb88",
   "metadata": {},
   "source": [
    "## Save the Posts Data in a CSV_File ( Give a name to it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8311057c-1450-4154-9c97-1a7b99be91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('{username}_posts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dba656-1c82-4723-8aa4-bb2b42cac74d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a398ee2-dbd4-4664-b3a1-d921d57341bf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d92da-1ab2-4c65-aff7-e3ae051c542d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0338463f-1280-411d-b8fe-54607ced35c3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc196fbc-9237-4104-ad29-4511d6f8bce9",
   "metadata": {},
   "source": [
    "# Fetch Comments of the Users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "809db562-3ce3-4bda-9c2f-a25d9ef39fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncClient(\n",
    "    headers={\"User-Agent\": \"Mozilla/5.0\"},\n",
    "    timeout=10.0,\n",
    "    follow_redirects=True\n",
    ")\n",
    "\n",
    "class log:\n",
    "    @staticmethod\n",
    "    def success(msg): print(\"[SUCCESS]\", msg)\n",
    "    @staticmethod\n",
    "    def error(msg): print(\"[ERROR]\", msg)\n",
    "\n",
    "\n",
    "def parse_user_comments(response: Response) -> Dict:\n",
    "    \"\"\"Parse user comments from user profile comment page\"\"\"\n",
    "    selector = Selector(response.text)\n",
    "    data = []\n",
    "\n",
    "    for box in selector.xpath(\"//div[@id='siteTable']/div[contains(@class, 'thing')]\"):\n",
    "        author = box.xpath(\"./@data-author\").get()\n",
    "        link = box.xpath(\"./@data-permalink\").get()\n",
    "\n",
    "        dislikes = box.xpath(\".//span[contains(@class, 'dislikes')]/@title\").get()\n",
    "        upvotes = box.xpath(\".//span[contains(@class, 'likes')]/@title\").get()\n",
    "        downvotes = box.xpath(\".//span[contains(@class, 'unvoted')]/@title\").get()\n",
    "\n",
    "        comment_body = \"\".join(\n",
    "            box.xpath(\".//div[contains(@class, 'usertext-body')]/div/p//text()\").getall()\n",
    "        ).replace(\"\\n\", \"\").strip()\n",
    "\n",
    "        data.append({\n",
    "            \"authorId\": box.xpath(\"./@data-author-fullname\").get(),\n",
    "            \"author\": author,\n",
    "            \"authorProfile\": f\"https://www.reddit.com/user/{author}\" if author else None,\n",
    "            \"commentId\": box.xpath(\"./@data-fullname\").get(),\n",
    "            \"commentLink\": f\"https://www.reddit.com{link}\" if link else None,\n",
    "            \"commentBody\": comment_body,\n",
    "            \"attachedCommentLinks\": box.xpath(\".//div[contains(@class, 'usertext-body')]/div/p/a/@href\").getall(),\n",
    "            \"publishingDate\": box.xpath(\".//time/@datetime\").get(),\n",
    "            \"dislikes\": int(dislikes) if dislikes else None,\n",
    "            \"upvotes\": int(upvotes) if upvotes else None,\n",
    "            \"downvotes\": int(downvotes) if downvotes else None,\n",
    "            \"replyTo\": {\n",
    "                \"postTitle\": box.xpath(\".//p[@class='parent']/a[@class='title']/text()\").get(),\n",
    "                \"postLink\": f\"https://www.reddit.com{box.xpath('.//p[@class=\\\"parent\\\"]/a[@class=\\\"title\\\"]/@href').get()}\",\n",
    "                \"postAuthor\": box.xpath(\".//p[@class='parent']/a[contains(@class, 'author')]/text()\").get(),\n",
    "                \"postSubreddit\": box.xpath(\"./@data-subreddit-prefixed\").get(),    \n",
    "            }\n",
    "        })\n",
    "\n",
    "    next_page_url = selector.xpath(\"//span[@class='next-button']/a/@href\").get()\n",
    "    return {\"data\": data, \"url\": next_page_url}\n",
    "\n",
    "\n",
    "async def scrape_user_comments(username: str, sort: Union[\"new\", \"top\", \"controversial\"] = \"new\", max_pages: int = None) -> List[Dict]:\n",
    "    \"\"\"Scrape comments from a Reddit user profile\"\"\"\n",
    "    url = f\"https://old.reddit.com/user/{username}/comments/?sort={sort}\"\n",
    "    response = await client.get(url)\n",
    "    parsed = parse_user_comments(response)\n",
    "    comment_data, next_page_url = parsed[\"data\"], parsed[\"url\"]\n",
    "\n",
    "    while next_page_url and (max_pages is None or max_pages > 0):\n",
    "        response = await client.get(next_page_url)\n",
    "        parsed = parse_user_comments(response)\n",
    "        comment_data.extend(parsed[\"data\"])\n",
    "        next_page_url = parsed[\"url\"]\n",
    "        if max_pages is not None:\n",
    "            max_pages -= 1\n",
    "\n",
    "    log.success(f\"Scraped {len(comment_data)} comments from {username}\")\n",
    "    return comment_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ffd50-6a12-40c5-ba83-87cfc55b74df",
   "metadata": {},
   "source": [
    "## Provide Username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc0577f4-4636-4519-8478-592023f20307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Scraped 10 comments from Hungry-Move-6603\n"
     ]
    }
   ],
   "source": [
    "reddit_username = \n",
    "comments = await scrape_user_comments(reddit_username, sort=\"new\", max_pages=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d7833-b071-42f1-a5e9-bf780488deb9",
   "metadata": {},
   "source": [
    "## Creating and Saving the Data Frame as a CSV_File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38432e3a-fda9-4c45-aa91-7b7e191ca274",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(comments)\n",
    "new_df.to_csv('{username}_comments.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bea2817e-1e73-4d81-9074-e11330067f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authorId</th>\n",
       "      <th>author</th>\n",
       "      <th>authorProfile</th>\n",
       "      <th>commentId</th>\n",
       "      <th>commentLink</th>\n",
       "      <th>commentBody</th>\n",
       "      <th>attachedCommentLinks</th>\n",
       "      <th>publishingDate</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>downvotes</th>\n",
       "      <th>replyTo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_bcxve1ah</td>\n",
       "      <td>Hungry-Move-6603</td>\n",
       "      <td>https://www.reddit.com/user/Hungry-Move-6603</td>\n",
       "      <td>t1_n2ybup0</td>\n",
       "      <td>https://www.reddit.com/r/nagpur/comments/1lyb0...</td>\n",
       "      <td>I was caught without helmet and license (close...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025-07-13T19:53:17+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'postTitle': 'A very odd experience', 'postLi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_bcxve1ah</td>\n",
       "      <td>Hungry-Move-6603</td>\n",
       "      <td>https://www.reddit.com/user/Hungry-Move-6603</td>\n",
       "      <td>t1_n2y7g0s</td>\n",
       "      <td>https://www.reddit.com/r/nagpur/comments/1lyb0...</td>\n",
       "      <td>Cops keep a civ around to discuss bribes.</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025-07-13T19:31:23+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'postTitle': 'A very odd experience', 'postLi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_bcxve1ah</td>\n",
       "      <td>Hungry-Move-6603</td>\n",
       "      <td>https://www.reddit.com/user/Hungry-Move-6603</td>\n",
       "      <td>t1_n2vkdpb</td>\n",
       "      <td>https://www.reddit.com/r/IndiaUnfilter/comment...</td>\n",
       "      <td>Toh hum Noida or Ghaziabad se pahadio ko bhaga...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025-07-13T10:31:41+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'postTitle': 'People are smoking hookah in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_bcxve1ah</td>\n",
       "      <td>Hungry-Move-6603</td>\n",
       "      <td>https://www.reddit.com/user/Hungry-Move-6603</td>\n",
       "      <td>t1_n2kh3aq</td>\n",
       "      <td>https://www.reddit.com/r/lucknow/comments/1lwb...</td>\n",
       "      <td>A menu easy to cook/process - healthy and quick.</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025-07-11T15:47:19+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'postTitle': 'Any Tiffin service providing hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_bcxve1ah</td>\n",
       "      <td>Hungry-Move-6603</td>\n",
       "      <td>https://www.reddit.com/user/Hungry-Move-6603</td>\n",
       "      <td>t1_n2ilsqh</td>\n",
       "      <td>https://www.reddit.com/r/lucknow/comments/1lwy...</td>\n",
       "      <td>Haha Delhi is hateable too but mostly those ar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025-07-11T08:46:21+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'postTitle': 'Everyone is something in LKO', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      authorId            author  \\\n",
       "0  t2_bcxve1ah  Hungry-Move-6603   \n",
       "1  t2_bcxve1ah  Hungry-Move-6603   \n",
       "2  t2_bcxve1ah  Hungry-Move-6603   \n",
       "3  t2_bcxve1ah  Hungry-Move-6603   \n",
       "4  t2_bcxve1ah  Hungry-Move-6603   \n",
       "\n",
       "                                  authorProfile   commentId  \\\n",
       "0  https://www.reddit.com/user/Hungry-Move-6603  t1_n2ybup0   \n",
       "1  https://www.reddit.com/user/Hungry-Move-6603  t1_n2y7g0s   \n",
       "2  https://www.reddit.com/user/Hungry-Move-6603  t1_n2vkdpb   \n",
       "3  https://www.reddit.com/user/Hungry-Move-6603  t1_n2kh3aq   \n",
       "4  https://www.reddit.com/user/Hungry-Move-6603  t1_n2ilsqh   \n",
       "\n",
       "                                         commentLink  \\\n",
       "0  https://www.reddit.com/r/nagpur/comments/1lyb0...   \n",
       "1  https://www.reddit.com/r/nagpur/comments/1lyb0...   \n",
       "2  https://www.reddit.com/r/IndiaUnfilter/comment...   \n",
       "3  https://www.reddit.com/r/lucknow/comments/1lwb...   \n",
       "4  https://www.reddit.com/r/lucknow/comments/1lwy...   \n",
       "\n",
       "                                         commentBody attachedCommentLinks  \\\n",
       "0  I was caught without helmet and license (close...                   []   \n",
       "1          Cops keep a civ around to discuss bribes.                   []   \n",
       "2  Toh hum Noida or Ghaziabad se pahadio ko bhaga...                   []   \n",
       "3   A menu easy to cook/process - healthy and quick.                   []   \n",
       "4  Haha Delhi is hateable too but mostly those ar...                   []   \n",
       "\n",
       "              publishingDate  dislikes  upvotes  downvotes  \\\n",
       "0  2025-07-13T19:53:17+00:00         0        0          1   \n",
       "1  2025-07-13T19:31:23+00:00         0        0          1   \n",
       "2  2025-07-13T10:31:41+00:00         0        0          1   \n",
       "3  2025-07-11T15:47:19+00:00         0        0          1   \n",
       "4  2025-07-11T08:46:21+00:00         0        0          1   \n",
       "\n",
       "                                             replyTo  \n",
       "0  {'postTitle': 'A very odd experience', 'postLi...  \n",
       "1  {'postTitle': 'A very odd experience', 'postLi...  \n",
       "2  {'postTitle': 'People are smoking hookah in th...  \n",
       "3  {'postTitle': 'Any Tiffin service providing hi...  \n",
       "4  {'postTitle': 'Everyone is something in LKO', ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the data frame\n",
    "new_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cfd545-dd88-486e-b85d-d9292b5b368b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea65f6-bbc5-4f17-9f4e-dceba02bbd88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4853aa-37ea-4add-9a07-4151e19e743c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8813a56c-3896-46ec-8010-89dceadfda76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634207e4-2de4-4408-aba9-d425805cf8b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
